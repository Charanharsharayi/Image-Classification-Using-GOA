# Make sure to install xgboost: pip install xgboost
# Make sure to install optuna: pip install optuna
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.svm import SVC # Kept for baseline comparison
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
import time
import joblib
from tqdm import tqdm
import optuna # Import Optuna
import os # Import os

# --- Constants for Persistence (Updated for XGB) ---
BEST_SOLUTION_FILE = "goa_best_solution_aid_xgb_v1.npz"
FINAL_MODEL_FILE = "final_xgb_model_aid_v1.joblib"


# --- Helper Functions ---

def sigmoid(x):
    """Sigmoid transfer function to map continuous values to probabilities."""
    # Add clip to prevent overflow in np.exp
    x_clipped = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x_clipped))

def get_fitness(X_train, y_train, X_val, y_val, feature_subset, num_classes, alpha=0.99):
    """
    Fitness function for GOA, now using a lightweight XGBoost for evaluation.
    It balances classification accuracy and the number of selected features.
    It trains on X_train and evaluates fitness on X_val.
    A lower fitness value is better.

    Args:
        X_train, y_train: The training dataset.
        X_val, y_val: The validation dataset (used for fitness evaluation).
        feature_subset (np.array): A binary array (0 or 1) indicating selected features.
        num_classes (int): The number of unique classes for XGBoost.
        alpha (float): A weight to control the balance between accuracy and feature reduction.

    Returns:
        float: The calculated fitness value.
    """
    num_selected = np.sum(feature_subset)
    total_features = len(feature_subset)

    # If no features are selected, return the worst possible fitness
    if num_selected == 0:
        return 1.0

    # Select the columns based on the feature subset
    selected_indices = np.where(feature_subset == 1)[0]
    X_train_subset = X_train[:, selected_indices]
    X_val_subset = X_val[:, selected_indices]

    # --- Use a lightweight XGBoost for speed during fitness evaluation ---
    # Using low n_estimators and max_depth to make it fast
    xgb_fitness = xgb.XGBClassifier(
        n_estimators=50,       # Low number of trees for speed
        max_depth=3,           # Shallow trees
        learning_rate=0.1,
        objective='multi:softmax', # Use multi:softmax for multi-class
        num_class=num_classes,
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42,
        n_jobs=1               # Limit to 1 job to avoid conflicts
    )
    
    xgb_fitness.fit(X_train_subset, y_train)
    
    # Evaluate accuracy on the VALIDATION set
    accuracy = xgb_fitness.score(X_val_subset, y_val)

    # Classification error rate
    error_rate = 1 - accuracy

    # The fitness function is a weighted sum of error rate and feature reduction rate
    fitness = alpha * error_rate + (1 - alpha) * (num_selected / total_features)
    
    return fitness

# --- Grasshopper Optimization Algorithm (GOA) for Feature Selection ---

def run_goa_feature_selection(X_train, y_train, X_val, y_val, num_classes, num_agents=30, max_iter=50):
    """
    Implements an enhanced Binary GOA with an anti-stagnation mechanism.
    This function now uses the validation set for fitness evaluation via XGBoost.

    Args:
        X_train, y_train: Training dataset.
        X_val, y_val: Validation dataset (for get_fitness).
        num_classes (int): The number of unique classes for XGBoost.
        num_agents (int): The number of grasshoppers in the swarm.
        max_iter (int): The maximum number of iterations for the algorithm.

    Returns:
        tuple: (best_feature_subset, target_position, target_fitness, convergence_curve)
    """
    num_features = X_train.shape[1]

    # GOA parameters
    c_max = 1
    c_min = 0.00004
    
    # Initialize grasshopper positions
    positions = np.random.rand(num_agents, num_features)
    
    # Initialize target (best solution)
    target_position = np.zeros(num_features)
    target_fitness = float('inf')
    
    convergence_curve = []
    stagnation_counter = 0
    stagnation_limit = 15 # Iterations without improvement before shake-up

    # --- Initial evaluation to find the first target solution ---
    for i in range(num_agents):
        binary_position = (sigmoid(positions[i, :]) > 0.5).astype(int)
        # Pass validation data and num_classes to get_fitness
        current_fitness = get_fitness(X_train, y_train, X_val, y_val, binary_position, num_classes)
        if current_fitness < target_fitness:
            target_fitness = current_fitness
            target_position = positions[i, :].copy()

    # --- Main GOA Loop ---
    iterator = tqdm(range(max_iter), desc="GOA Iterations", leave=False)
    for t in iterator:
        c = c_max - t * ((c_max - c_min) / max_iter)
        
        # Store previous best fitness to check for improvement
        prev_fitness = target_fitness

        new_positions = np.zeros_like(positions)
        for i in range(num_agents):
            S_i = np.zeros(num_features)
            for j in range(num_agents):
                if i != j:
                    dist = np.linalg.norm(positions[i, :] - positions[j, :])
                    r_ij_vec = (positions[j, :] - positions[i, :]) / (dist + 1e-8)
                    f = 0.5
                    l = 1.5
                    s_r = f * np.exp(-dist / l) - np.exp(-dist)
                    S_i += s_r * r_ij_vec
            
            temp_position = c * S_i + target_position
            new_positions[i, :] = temp_position

        positions = np.clip(new_positions, 0, 1)

        for i in range(num_agents):
            binary_position = (sigmoid(positions[i, :]) > 0.5).astype(int)
            # Pass validation data and num_classes to get_fitness
            current_fitness = get_fitness(X_train, y_train, X_val, y_val, binary_position, num_classes)
            if current_fitness < target_fitness:
                target_fitness = current_fitness
                target_position = positions[i, :].copy()
        
        convergence_curve.append(target_fitness)
        iterator.set_postfix(best_fitness=f"{target_fitness:.6f}")

        # --- Anti-Stagnation Mechanism ---
        if target_fitness >= prev_fitness:
            stagnation_counter += 1
        else:
            stagnation_counter = 0 # Reset if improvement is found

        if stagnation_counter >= stagnation_limit:
            tqdm.write(f"Stagnation detected at iteration {t+1}. Applying population shake-up.")
            num_to_perturb = int(num_agents * 0.3) # Shake up 30%
            perturb_indices = np.random.choice(num_agents, num_to_perturb, replace=False)
            for idx in perturb_indices:
                if not np.array_equal(positions[idx], target_position):
                    noise = np.random.normal(0, 0.1, num_features)
                    positions[idx] += noise
            positions = np.clip(positions, 0, 1)
            stagnation_counter = 0

    best_feature_subset = (sigmoid(target_position) > 0.5).astype(int)
    print("GOA Run Finished.")
    return best_feature_subset, target_position, target_fitness, convergence_curve


# --- Baseline Models ---

def run_baseline_model_svm(X_train, y_train, X_test, y_test, target_names):
    """Trains and evaluates a baseline SVM classifier on the full feature set.
       Trains on X_train, evaluates on X_test."""
    print("\n--- Training Baseline Model (SVM with All Features) ---")
    model = SVC(kernel='rbf', C=1.0, random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    print("\nBaseline SVM Performance (on Test Set):")
    print(f"Number of features used: {X_train.shape[1]}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))

def run_baseline_model_xgb(X_train, y_train, X_test, y_test, num_classes, target_names):
    """Trains and evaluates a baseline XGBoost classifier on the full feature set.
       Trains on X_train, evaluates on X_test."""
    print("\n--- Training Baseline Model (XGBoost with All Features) ---")
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        objective='multi:softmax',
        num_class=num_classes,
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    print("\nBaseline XGBoost Performance (on Test Set):")
    print(f"Number of features used: {X_train.shape[1]}")
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))
    

# --- Main Classification Function ---

def run_classification(dataset_path, num_runs=10):
    """
    Main function to load data, run GOA over multiple runs, save the best solution,
    and train the final XGBoost model.
    
    Now handles loading from a single .npz file or walking a directory
    for multiple .npz files.
    """
    print(f"Loading dataset from: {dataset_path}")
    all_features = []
    all_labels = []

    try:
        if os.path.isfile(dataset_path) and dataset_path.endswith('.npz'):
            # Case 1: Path is a single .npz file
            print(f"Loading single file: {dataset_path}")
            data = np.load(dataset_path)
            if 'features' in data and 'labels' in data:
                all_features.append(data['features'])
                all_labels.append(data['labels'])
            else:
                print(f"Error: File {dataset_path} is missing 'features' or 'labels'.")
                return

        elif os.path.isdir(dataset_path):
            # Case 2: Path is a directory
            print(f"'{dataset_path}' is a directory. Walking directory to load .npz files...")
            for root, dirs, files in os.walk(dataset_path):
                # Sort files to ensure a consistent order (optional but good practice)
                for file in sorted(files): 
                    if file.endswith('.npz'):
                        file_path = os.path.join(root, file)
                        try:
                            data = np.load(file_path)
                            if 'features' in data and 'labels' in data:
                                print(f"  Loading {file_path}...")
                                all_features.append(data['features'])
                                all_labels.append(data['labels'])
                            else:
                                print(f"  Skipping {file_path} (missing 'features' or 'labels').")
                        except Exception as e:
                            print(f"  Error loading {file_path}: {e}")
            
        else:
            print(f"Error: Path '{dataset_path}' is not a valid .npz file or directory.")
            return

        # Check if we loaded any data
        if not all_features:
            print("Error: No valid .npz files with 'features' and 'labels' were loaded.")
            return
            
        # Concatenate all loaded data
        X = np.concatenate(all_features, axis=0)
        y = np.concatenate(all_labels, axis=0)

    except FileNotFoundError:
        print(f"Error: Dataset path not found at '{dataset_path}'.")
        return
    except Exception as e:
        print(f"An unexpected error occurred while loading data: {e}")
        return

    # --- CRITICAL: Encode labels for XGBoost ---
    # XGBoost requires integer labels starting from 0
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    num_classes = len(le.classes_)
    
    print(f"\nDataset loaded: {X.shape[0]} samples, {X.shape[1]} features, {num_classes} classes.")
    print(f"Original labels: {le.classes_}")
    print(f"Using encoded labels (0 to {num_classes-1}) for classification.")
    
    # --- Create 70:10:20 (Train:Validation:Test) Split using encoded labels ---
    X_train_val, X_test, y_train_val, y_test = train_test_split(
        X, y_encoded, test_size=0.20, random_state=42, stratify=y_encoded
    )
    
    X_train, X_val, y_train, y_val = train_test_split(
        X_train_val, y_train_val, test_size=0.125, random_state=42, stratify=y_train_val
    )
    
    print(f"\nData split:")
    print(f"  Training set:   {X_train.shape[0]} samples ({len(X_train)/len(X)*100:.1f}%)")
    print(f"  Validation set: {X_val.shape[0]} samples ({len(X_val)/len(X)*100:.1f}%)")
    print(f"  Test set:       {X_test.shape[0]} samples ({len(X_test)/len(X)*100:.1f}%)")

    # Scale data based on the TRAINING set only
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    # Also scale the combined train+val set for final training
    X_train_val_scaled = scaler.transform(X_train_val)

    # --- Run Baseline Models ---
    # (trains on train, evaluates on test)
    run_baseline_model_svm(X_train_scaled, y_train, X_test_scaled, y_test, target_names=le.classes_)
    run_baseline_model_xgb(X_train_scaled, y_train, X_test_scaled, y_test, num_classes, target_names=le.classes_)

    # Load previous best GOA solution if it exists
    overall_best_position = None
    overall_best_fitness = float('inf')
    try:
        data = np.load(BEST_SOLUTION_FILE)
        overall_best_position = data['position']
        overall_best_fitness = data['fitness']
        print(f"\nLoaded previously best solution with fitness: {overall_best_fitness:.6f}")
    except FileNotFoundError:
        print("\nNo previous GOA solution found. Starting from scratch.")

    # Run GOA for several "epochs" to find the best feature subset
    start_time_goa = time.time()
    all_run_curves = []
    
    run_iterator = tqdm(range(num_runs), desc="Overall GOA Progress")
    for run in run_iterator:
        run_iterator.set_description(f"GOA Run {run + 1}/{num_runs}")
        # Pass validation data AND num_classes to GOA for optimization
        _, position, fitness, convergence = run_goa_feature_selection(
            X_train_scaled, y_train, X_val_scaled, y_val, num_classes, num_agents=30, max_iter=60
        )
        all_run_curves.append(convergence)
        
        if fitness < overall_best_fitness:
            tqdm.write(f"New best solution found in Run {run + 1}! Fitness improved from {overall_best_fitness:.6f} to {fitness:.6f}")
            overall_best_fitness = fitness
            overall_best_position = position

    end_time_goa = time.time()
    print(f"\nTotal GOA execution time for {num_runs} runs: {end_time_goa - start_time_goa:.2f} seconds")

    # Save the new overall best solution for future runs
    if overall_best_position is not None:
        print(f"\nSaving new best solution to '{BEST_SOLUTION_FILE}'")
        np.savez_compressed(BEST_SOLUTION_FILE, position=overall_best_position, fitness=overall_best_fitness)
        best_features = (sigmoid(overall_best_position) > 0.5).astype(int)
    else:
        print("GOA failed to find any valid solution.")
        return

    # --- Train the final XGBoost model using the overall best features ---
    print("\n--- Tuning and Training Final Model (XGBoost with GOA-Selected Features) ---")
    num_selected_features = np.sum(best_features)
    
    if num_selected_features == 0:
        print("\nWarning: GOA did not select any features. Cannot train final model.")
        return
        
    selected_indices = np.where(best_features == 1)[0]
    
    # Get the training set with selected features for hyperparameter tuning
    X_train_for_tuning = X_train_scaled[:, selected_indices]
    
    # --- 1. Hyperparameter Tuning using Optuna on TRAINING data ---
    print(f"\nStarting Optuna study to find best params for {num_selected_features} features...")

    def objective(trial):
        """Optuna objective function to find best XGBoost hyperparameters."""
        
        # Define the hyperparameter search space
        param = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 500),
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),
        }

        # Create the XGBoost classifier with trial parameters
        model = xgb.XGBClassifier(
            objective='multi:softmax',
            num_class=num_classes,
            eval_metric='mlogloss',
            use_label_encoder=False,
            random_state=42,
            n_jobs=-1, # Use all cores for training this trial
            **param
        )
        
        # Use 5-fold cross-validation, just like GridSearchCV
        # We use the training data (X_train_for_tuning) for this
        score = cross_val_score(
            model, 
            X_train_for_tuning, 
            y_train, 
            cv=5, 
            scoring='accuracy',
            n_jobs=1 # cross_val_score n_jobs
        ).mean()
        
        return score

    # Suppress informational messages from Optuna
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    
    # Create an Optuna study and optimize
    study = optuna.create_study(direction='maximize')
    study.optimize(
        objective, 
        n_trials=50,  # Number of trials to run (e.g., 50-100)
        show_progress_bar=True # Show a tqdm progress bar
    )

    print(f"\nOptuna study finished.")
    best_params = study.best_params
    best_score = study.best_value
    print(f"Best parameters found: {best_params}")
    print(f"Best cross-validation accuracy: {best_score:.4f}")

    # --- 2. Train Final Model on COMBINED (Train+Val) Data ---
    print("\nTraining final model on combined (Train+Val) data using best parameters...")
    
    # Get the combined train+val data with selected features
    X_train_val_final = X_train_val_scaled[:, selected_indices]
    
    # Evaluate on the (unseen) TEST set
    X_test_final = X_test_scaled[:, selected_indices]

    # Create the final model with the best parameters found
    # Create the final model with the best parameters found by Optuna
    final_model = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=num_classes,
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42,
        **best_params # Use the best parameters from the Optuna study
    )
    
    # Train on the full (train+val) dataset
    final_model.fit(X_train_val_final, y_train_val)
    
    y_pred_final = final_model.predict(X_test_final)

    print("\nFinal GOA-XGBoost Model Performance (on Test Set):")
    print(f"Number of features selected by GOA: {num_selected_features} (out of {X.shape[1]})")
    print(f"Accuracy: {accuracy_score(y_test, y_pred_final):.4f}")
    print("\nClassification Report:")
    # Use target_names to show original class labels
    print(classification_report(y_test, y_pred_final, target_names=le.classes_, zero_division=0))
    
    print(f"\nSaving final trained model to '{FINAL_MODEL_FILE}'")
    joblib.dump(final_model, FINAL_MODEL_FILE)
    
    # Plot the convergence curves for all runs
    plt.figure(figsize=(12, 7))
    for i, curve in enumerate(all_run_curves):
        plt.plot(curve, alpha=0.7, label=f'Run {i+1}')
    
    if len(all_run_curves) > 0:
        avg_curve = np.mean(all_run_curves, axis=0)
        plt.plot(avg_curve, color='black', linewidth=2, linestyle='--', label='Average')
        
    plt.title(f"GOA Convergence Curves ({num_runs} Runs) - XGB Fitness")
    plt.xlabel("Iteration")
    plt.ylabel("Best Fitness Value (on Validation Set)")
    plt.grid(True)
    if len(all_run_curves) > 1:
        plt.legend() # Only show legend if there are multiple runs
    
    plt.savefig("goa_convergence_runs_xgb_v1.png")
    print("\nConvergence curve plot for all runs saved as 'goa_convergence_runs_xgb_v1.png'")


if __name__ == '__main__':
    # --- Time vs. Quality Parameters ---
    # You can adjust these values to control the runtime.
    # More runs and more iterations will take longer but may find a better solution.
    
    # For a quick test:
    # RUNS = 3
    
    # For a thorough search (increased from 5):
    RUNS = 8
    # MAX_ITERATIONS is now set inside run_goa_feature_selection call (60)
    
    # --- IMPORTANT ---
    # Change this to the path of your directory containing .npz files
    # or to a single .npz file.
    DATA_PATH = "features_aid" # e.g., "features_aid" directory or "features_aid.npz" file
    
    run_classification(DATA_PATH, num_runs=RUNS)
    